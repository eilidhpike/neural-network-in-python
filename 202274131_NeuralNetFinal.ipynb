{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Part 2: Neural Net Template\n",
    "\n",
    "This file contains the template code for the Neural Net with hidden layers.\n",
    "\n",
    "### Artificial Neural Net Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Complete the initialisation of the neural net. <br>\n",
    "#### 2.3. Complete the training implementation.<br>\n",
    "#### 2.4. Complete the testing implementation <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN:\n",
    "\n",
    "    #==========================================#\n",
    "    # The init method is called when an object #\n",
    "    # is created. It can be used to initialize #\n",
    "    # the attributes of the class.             #\n",
    "    #==========================================#\n",
    "    def __init__(self, no_inputs, no_outputs, hidden_layers=[3,4,5], max_iterations=20, learning_rate=0.1):\n",
    "\n",
    "        self.no_inputs = no_inputs #input layer size\n",
    "        self.no_outputs = no_outputs #output layer size\n",
    "        self.no_hidden_layers = len(hidden_layers) #number of hidden layers in the network\n",
    "        self.hidden_layers = hidden_layers #parameter\n",
    "        self.weights = [] #weights for each layer\n",
    "        self.biases = []  #biases for each layer                     \n",
    "\n",
    "        for layer in range(self.no_hidden_layers+1): #initialise weights and biases\n",
    "            if layer == 0: #first layer\n",
    "                no_nodes = self.hidden_layers[0] \n",
    "                no_inputs_to_layer = no_inputs\n",
    "            elif layer == self.no_hidden_layers:\n",
    "                no_nodes = self.no_outputs\n",
    "                no_inputs_to_layer = self.hidden_layers[-1] #output layer\n",
    "            else:\n",
    "                no_nodes = self.hidden_layers[layer] #nodes same number as previous layer\n",
    "                no_inputs_to_layer = self.hidden_layers[layer-1]\n",
    "                                 \n",
    "            # initialise weight matrix of shape: (no_nodes, no_inputs_to_layer)\n",
    "            weights = np.random.randn(no_nodes, no_inputs_to_layer)\n",
    "            self.weights.append(weights) #add the updated weight matrix to the network\n",
    "            # adding the bias vector \n",
    "            biases = np.zeros((1, no_nodes))\n",
    "            self.biases.append(biases) #add to the updated biases to the network\n",
    "        \n",
    "        self.max_iterations = max_iterations\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    #===================================#\n",
    "    # Performs the activation function. #\n",
    "    # Expects an array of values of     #\n",
    "    # shape (1,N) where N is the number #\n",
    "    # of nodes in the layer.            #\n",
    "    #===================================#\n",
    "    def activate(self, a): #Sigmoid\n",
    "        return 1 / (1 + np.exp(-a))\n",
    "    \n",
    "    \n",
    "\n",
    "    #===============================#\n",
    "    # Trains the net using labelled #\n",
    "    # training data.                #\n",
    "    #===============================#\n",
    "    def train(self, training_data, labels, batch_size=10):\n",
    "        n = len(training_data)\n",
    "        for i in range(0, n, batch_size):\n",
    "            batch_inputs = training_data[i:i+batch_size] #appending batch inputs to batch size parameter\n",
    "            batch_labels = labels[i:i+batch_size] #appending batch labels to corresponding imput\n",
    "        \n",
    "            # feed forward propogation\n",
    "            activation = batch_inputs.T\n",
    "            outputs = [activation]\n",
    "            for j in range(self.no_hidden_layers+1):\n",
    "                z = np.dot(self.weights[j], activation) + self.biases[j].T #weighted sum of the previous layer matrices and the bias term for current layer\n",
    "                activation = self.activate(z) #weights then sum the activation for that layer\n",
    "                outputs.append(activation) #append the activation value for current layer\n",
    "            error = outputs[-1] - batch_labels.T # calculate output layer error\n",
    "\n",
    "            # backpropagation\n",
    "            partial_derivatives = [np.dot(error, outputs[-2].T)] #calculated by the actual and predicted outcomes\n",
    "            for j in range(self.no_hidden_layers, 0, -1):\n",
    "                error = np.dot(self.weights[j].T, error) * outputs[j] * (1 - outputs[j]) #backpropgating the error signal by multiplying the transpose of the weight matrix\n",
    "                partial_derivatives.append(np.dot(error, outputs[j-1].T)) #partial derviatives appeneded at each layer\n",
    "            partial_derivatives.reverse()\n",
    "\n",
    "            # update weights and biases\n",
    "            for j in range(self.no_hidden_layers+1):\n",
    "                self.biases[j] -= self.learning_rate * np.sum(error, axis=1, keepdims=True) # updating biases \n",
    "                self.biases[j] = self.biases[j].T #biases transposed back to match the dimensions of orginal bias vector\n",
    "    \n",
    "    def test(self, testing_data, labels):\n",
    "        assert len(testing_data) == len(labels)\n",
    "        testing_data = np.concatenate((testing_data, np.ones((len(testing_data), 1))), axis=1)\n",
    "        true_positives = 0\n",
    "        true_negatives = 0\n",
    "        false_positives = 0\n",
    "        false_negatives = 0\n",
    "        for x, y in zip(testing_data, labels): \n",
    "            output = self.predict(x)\n",
    "            if np.argmax(output) == np.argmax(y):\n",
    "                if np.argmax(output) == 1:\n",
    "                    true_positives += 1\n",
    "                else:\n",
    "                    true_negatives += 1\n",
    "            else:\n",
    "                if np.argmax(output) == 1:\n",
    "                    false_positives += 1\n",
    "                else:\n",
    "                    false_negatives += 1\n",
    "\n",
    "        accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives)\n",
    "        precision = true_positives / (true_positives + false_positives)\n",
    "        recall = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "        print(\"Accuracy:\\t\"+str(accuracy))\n",
    "        print(\"Precision:\\t\"+str(precision))\n",
    "        print(\"Recall:\\t\"+str(recall))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Complete the implementation of the main method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data and test dataset\n",
    "\n",
    "image_size = 28 # width and length\n",
    "no_of_different_labels = 10 #  i.e. 0, 1, 2, 3, ..., 9\n",
    "image_pixels = image_size * image_size\n",
    "\n",
    "train_data = np.loadtxt(\"fashion_mnist_train.csv\", delimiter=\",\")\n",
    "test_data = np.loadtxt(\"fashion_mnist_test.csv\", delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multiplying each pixel by 0.99 / 255 and adding 0.1 to the result to avoid 0 values as imputs, which prevent weight updates. \n",
    "fac = 0.99 / 255\n",
    "train_imgs = np.asfarray(train_data[:, 1:]) * fac + 0.01\n",
    "test_imgs = np.asfarray(test_data[:, 1:]) * fac + 0.01\n",
    "\n",
    "train_labels = np.asfarray(train_data[:, :1])\n",
    "test_labels = np.asfarray(test_data[:, :1])\n",
    "\n",
    "#One hot encoding for labels \n",
    "lr = np.arange(10)\n",
    "\n",
    "#Changing labelled images into one-hot representations. \n",
    "train_labels_one_hot = (lr==train_labels).astype(float)\n",
    "test_labels_one_hot = (lr==test_labels).astype(float)\n",
    "\n",
    "train_labels_one_hot[train_labels_one_hot==0] = 0.01\n",
    "train_labels_one_hot[train_labels_one_hot==1] = 0.99\n",
    "test_labels_one_hot[test_labels_one_hot==0] = 0.01\n",
    "test_labels_one_hot[test_labels_one_hot==1] = 0.99\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "(784, 10)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (1,100) doesn't match the broadcast shape (100,100)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m training_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([np\u001b[38;5;241m.\u001b[39mappend([\u001b[38;5;241m1\u001b[39m], d[\u001b[38;5;241m1\u001b[39m:]) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m train_imgs])\n\u001b[0;32m      9\u001b[0m labels \u001b[38;5;241m=\u001b[39m train_labels_one_hot\n\u001b[1;32m---> 10\u001b[0m \u001b[43mannette\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Testing the neural network\u001b[39;00m\n",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36mANN.train\u001b[1;34m(self, training_data, labels, batch_size)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# update weights and biases\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mno_hidden_layers\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 83\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases[j] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msum(error, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases[j] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases[j]\u001b[38;5;241m.\u001b[39mT\n",
      "\u001b[1;31mValueError\u001b[0m: non-broadcastable output operand with shape (1,100) doesn't match the broadcast shape (100,100)"
     ]
    }
   ],
   "source": [
    "annette = ANN(no_inputs=image_pixels,\n",
    "                        no_outputs=10,\n",
    "                        hidden_layers=[100],\n",
    "                        learning_rate=0.001)\n",
    "\n",
    "# Training the neural network\n",
    "print(\"Training...\")\n",
    "training_data = np.array([np.append([1], d[1:]) for d in train_imgs])\n",
    "labels = train_labels_one_hot\n",
    "annette.train(training_data, labels)\n",
    "print(\"Complete.\")\n",
    "\n",
    "# Testing the neural network\n",
    "print(\"Testing...\")\n",
    "testing_data = np.array([np.append([1], d[1:]) for d in test_imgs])\n",
    "labels = test_labels_one_hot\n",
    "annette.test(testing_data, labels)\n",
    "print(\"Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 - Implement the rectifier activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU_ANN:\n",
    "\n",
    "    #==========================================#\n",
    "    # The init method is called when an object #\n",
    "    # is created. It can be used to initialize #\n",
    "    # the attributes of the class.             #\n",
    "    #==========================================#\n",
    "    def __init__(self, no_inputs, no_outputs, hidden_layers=[3,4,5], max_iterations=20, learning_rate=0.1):\n",
    "\n",
    "        self.no_inputs = no_inputs \n",
    "        self.no_outputs = no_outputs #number of nodes in the output layer\n",
    "        self.no_hidden_layers = len(hidden_layers) #number of hidden layers in the network\n",
    "        self.hidden_layers = hidden_layers #parameter\n",
    "        self.weights = [] #weights for each layer\n",
    "        self.biases = []  #biases for each layer                     \n",
    "\n",
    "        for layer in range(self.no_hidden_layers+1): #initialise weights and biases\n",
    "            if layer == 0: #first layer\n",
    "                no_nodes = self.hidden_layers[0] \n",
    "                no_inputs_to_layer = no_inputs\n",
    "            elif layer == self.no_hidden_layers:\n",
    "                no_nodes = self.no_outputs\n",
    "                no_inputs_to_layer = self.hidden_layers[-1] #output layer\n",
    "            else:\n",
    "                no_nodes = self.hidden_layers[layer] #nodes same number as previous layer\n",
    "                no_inputs_to_layer = self.hidden_layers[layer-1]\n",
    "                                 \n",
    "            # initialise weight matrix of shape: (no_nodes, no_inputs_to_layer)\n",
    "            weights = np.random.randn(no_nodes, no_inputs_to_layer)\n",
    "            self.weights.append(weights)\n",
    "            \n",
    "            biases = np.zeros((1, no_nodes))\n",
    "            self.biases.append(biases)\n",
    "        \n",
    "        self.max_iterations = max_iterations\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    #===================================#\n",
    "    # Performs the activation function. #\n",
    "    # Expects an array of values of     #\n",
    "    # shape (1,N) where N is the number #\n",
    "    # of nodes in the layer.            #\n",
    "    #===================================#\n",
    "    def activate(self, a): #ReLU\n",
    "        return np.maximum(0,a)\n",
    "\n",
    "    #===============================#\n",
    "    # Trains the net using labelled #\n",
    "    # training data.                #\n",
    "    #===============================#\n",
    "    def train(self, training_data, labels, batch_size=10):\n",
    "        n = len(training_data)\n",
    "        for i in range(0, n, batch_size):\n",
    "            batch_inputs = training_data[i:i+batch_size]\n",
    "            batch_labels = labels[i:i+batch_size]\n",
    "        \n",
    "            # feed forward\n",
    "            activation = batch_inputs.T\n",
    "            outputs = [activation]\n",
    "            print(activation.shape)\n",
    "            for j in range(self.no_hidden_layers+1):\n",
    "                z = np.dot(self.weights[j], activation) + self.biases[j].T #weighted sum of the previous layers\n",
    "                activation = self.activate(z)\n",
    "                outputs.append(activation)\n",
    "                \n",
    "            # calculate output layer error\n",
    "            error = outputs[-1] - batch_labels.T\n",
    "\n",
    "            # backpropagation\n",
    "            partial_derivatives = [np.dot(error, outputs[-2].T)]\n",
    "            for j in range(self.no_hidden_layers, 0, -1):\n",
    "                error = np.dot(self.weights[j].T, error) * (outputs[j] > 0)\n",
    "                partial_derivatives.append(np.dot(error, outputs[j-1].T))\n",
    "            partial_derivatives.reverse()\n",
    "\n",
    "            # update weights and biases\n",
    "            for j in range(self.no_hidden_layers+1):\n",
    "                self.biases[j] -= self.learning_rate * np.sum(error, axis=1, keepdims=True)\n",
    "                self.biases[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ReLU = ReLU_ANN(no_inputs=image_pixels,\n",
    "                        no_outputs=10,\n",
    "                        hidden_layers=[100],\n",
    "                        learning_rate=0.001)\n",
    "\n",
    "# Training the neural network\n",
    "print(\"Training...\")\n",
    "training_data = np.array([np.append([1], d[1:]) for d in train_imgs])\n",
    "labels = train_labels_one_hot\n",
    "ReLU.train(training_data, labels)\n",
    "print(\"Complete.\")\n",
    "\n",
    "# Testing the neural network\n",
    "print(\"Testing...\")\n",
    "testing_data = np.array([np.append([1], d[1:]) for d in test_imgs])\n",
    "labels = test_labels_one_hot\n",
    "ReLU.test(testing_data, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
